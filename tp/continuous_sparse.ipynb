{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c168a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af911d",
   "metadata": {},
   "source": [
    "# Intro: Sparse recovery and ISTA\n",
    "\n",
    "We'll start by shortly testing the Iterative Soft Thresholding Algorithm (ISTA) for sparse recovery. This algorithm aims to solve the Basis Pursuit Denoising problem:\n",
    "$$\n",
    "\\min_z \\frac{1}{2} \\| y - Mz\\|_2^2 + \\lambda \\|z\\|_1\n",
    "$$\n",
    "\n",
    "One way (there are many) to see the ISTA algorithm is as an *alternate gradient descent* on each term of the cost function. The first term is quadratic and easy. However, the $\\ell_1$ norm in the second term is convex but not differentiable! Hence we replace the gradient descent step on this term with a so-called *proximal operator*, defined for any convex function $f$ as:\n",
    "$$\n",
    "\\text{prox}_f(x) = \\min_z \\frac12 \\| x-z \\|_2^2 + f(z)\n",
    "$$\n",
    "\n",
    "**Q**: when $f$ is indeed differentiable, to what corresponds the operation $z^{(t+1)} = \\text{prox}_{\\eta f}(z^{(t)})$ for a small $\\eta>0$ ?\n",
    "\n",
    "Combining the two descent steps, the ISTA algorithm reads:\n",
    "\\begin{align*}\n",
    "\\hat z^{(t+1)} &= z^{(t)} - \\eta M^\\top (Mz^{(t)} - y) \\\\\n",
    "z^{(t+1)} &= \\text{prox}_{\\eta' \\lambda \\|\\cdot\\|_1} (\\hat z^{(t+1)})\n",
    "\\end{align*}\n",
    "for small $\\eta, \\eta'$ (note that $\\eta'$ can be integrated in $\\lambda$).\n",
    "\n",
    "**Q**: show that $\\text{prox}_{\\lambda \\|\\cdot\\|_1}(x)$ is the Soft-Thresholding operator\n",
    "$$\n",
    "ST_\\lambda(x) = \\begin{cases}\n",
    "x + \\lambda &\\text{if $x \\leq -\\lambda$} \\\\\n",
    "0 &\\text{if $-\\lambda \\leq x \\leq \\lambda$} \\\\\n",
    "x - \\lambda &\\text{if $x \\geq \\lambda$}\n",
    "\\end{cases}\n",
    "$$\n",
    "Implement it simply by remarking that $ST_\\lambda(x) = \\text{sign}(x)\\max(|x|-\\lambda, 0)$ and plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf33e4",
   "metadata": {},
   "source": [
    "**Exo**: implement the BPD sparse recovery problem with a sparse vector $x$, a Gaussian iid measurement matrix $M$, and noiseless or noisy measurements. Test different dimensions $n,m,s$ and regularization parameter $\\lambda$. Use pyplot's stem function for better visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba2cc5",
   "metadata": {},
   "source": [
    "# Main: Continuous sparsity\n",
    "\n",
    "In this practical session, we'll look at some continuous sparsity problem. Be aware that this is still a very much active area of research, the algorithms are imperfect and often need some tuning !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfbf36",
   "metadata": {},
   "source": [
    "You need to install torch! See here: https://pytorch.org/get-started/locally/ (install CPU only on laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2778aa2",
   "metadata": {},
   "source": [
    "## Fourier measurements on an interval\n",
    "\n",
    "We start with the framework from \"Off-the-grid compressed sensing\" by Tang and Recht. In this paper,\n",
    "the observation model is:\n",
    "    $$\n",
    "    y= [\\sum_{k=1}^s c_k e^{i w_k j}]_{j \\in T}\n",
    "    $$\n",
    "for some indices $T \\subset \\{0,\\ldots, n-1\\}$ of size $m$ and unknown amplitudes $c_k \\in \\mathbb{R}$ and frequencies $w_k \\in [1,0]$.\n",
    "    \n",
    "Equivalently,\n",
    "    $$\n",
    "    y = \\int_0^1 \\phi(w) d\\mu(w)\n",
    "    $$\n",
    "for a sparse Radon measure\n",
    "    $$\n",
    "    \\mu = \\sum_{k=1}^s c_k \\delta_{w_k}\n",
    "    $$\n",
    "and a measuring function $\\phi(w) = [e^{i w j}]_{j \\in T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e42d89d",
   "metadata": {},
   "source": [
    "We give some utilities to implement the observation model. Here, instead of manipulating complex numbers (not easy in torch), we prefer to separate real and imaginary part. The stored $y$ will therefore be in $\\mathbb{R}^{2m}$ instead of $\\mathbb{C}^m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat(w, T):\n",
    "    \"\"\"\n",
    "        given positions, return a 2ind * s matrix containing the phi(w_k) as columns.\n",
    "        The 2 factor is due to real vs complex.\n",
    "    \"\"\"\n",
    "    return torch.cat((\n",
    "            torch.cos(T[:,None] * w[None, :]),\n",
    "            torch.sin(T[:,None] * w[None, :])))\n",
    "\n",
    "def obs_model(w, c, T):\n",
    "    \"\"\"\n",
    "        given positions and amplitudes, return the observed signal\n",
    "    \"\"\"\n",
    "    return (mat(w, T) @ c.view(-1,1)).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb7894",
   "metadata": {},
   "source": [
    "**Exo**: Given $n=400$ and $m=|T|=100$ indices selected randomly, generate the observed signal for $s=3$, choosing $c_k$ and $w_k$ as you please. Take $c_k$ sufficiently large (SNR). Plot the real part of $y$. Plot the full signal if we observed the full $T=\\{0, \\ldots, n-1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0a1fe",
   "metadata": {},
   "source": [
    "### Solving on a grid\n",
    "\n",
    "We will first start by solving this problem on a grid. Given a frequency $f$, we consider frequency bins:\n",
    "$$\n",
    "F = \\{1/f, 2/f, \\ldots, 1\\}\n",
    "$$\n",
    "With this model, we have approximately:\n",
    "$$\n",
    "y \\approx M x\n",
    "$$\n",
    "with an approximately sparse $x \\in \\mathbb{R}^{|F|}$ containing the $c_k$ at indices $p_k \\approx f w_k$, and $M$ containing the $\\phi(1/f), \\ldots \\phi(1)$ as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21f3c8",
   "metadata": {},
   "source": [
    "**Q**: Take $f=100$ and create the matrix $M$. Using the previous section, apply the ISTA algorithm to recover a sparse $x$. Compare the recovered positions within $F$ and the true ones $w_k$. (be careful that here we work with torch tensors, while the previous section worked with numpy. Use the utility .numpy() to convert a tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1a82c",
   "metadata": {},
   "source": [
    "**Q**: What do you observe? How thin must the grid be? What can you conclude on the coherence of the observation matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37ea13",
   "metadata": {},
   "source": [
    "### Simple improvement by non-convex optimization\n",
    "\n",
    "A simple way to improve these results is to select a few Diracs on the grid among the ISTA solution, and use to initialize an optimization problem. The non-convex formulation reads:\n",
    "$$\n",
    "\\min_{w,c} \\|y - \\sum_{k=1}^s c_k \\phi(w_k)\\|_2^2\n",
    "$$\n",
    "\n",
    "We will perform this optimization with Pytorch, using the Adam optimizer (a gradient descent with momentum). We give the optimization function below.\n",
    "\n",
    "**Q**: comment the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_positions(y, T, init_w, init_c, n_iter=2000, lr=.001):\n",
    "    w = torch.tensor(init_w, requires_grad=True, dtype=torch.float)\n",
    "    c = torch.tensor(init_c, requires_grad=True, dtype=torch.float)\n",
    "    optimizer = Adam([w,c], lr=lr)\n",
    "    iterates = []\n",
    "    for _ in range(n_iter):\n",
    "        loss = torch.nn.MSELoss()(y, obs_model(w, c, T))\n",
    "        iterates.append(loss.item()) # store loss for debugging\n",
    "        optimizer.zero_grad() # reinitialize gradients\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # perform one step of gradient descent\n",
    "        with torch.no_grad():\n",
    "            w.clamp_(0,1) # help optimization by clamping positions between 0 and 1\n",
    "    return w.detach(), c.detach(), iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f3460",
   "metadata": {},
   "source": [
    "**Q**: try to play with the parameters of the problem. Is non-convex optimization easy? Try to initialize the optimization with more Diracs extracted from the ISTA solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525191b",
   "metadata": {},
   "source": [
    "## Sketched learning\n",
    "\n",
    "In sketched learning, the \"integral\" is taken as an empirical average over some observed data points. The \"inverse\" problem is then to recover the distribution $P$ of the data. Given $z_1, \\ldots, z_N$ identically and independently distributed according to $P$, the law of large numbers give\n",
    "$$\n",
    "y = \\frac{1}{N} \\sum_{i=1}^n \\psi(z_i) \\approx \\int \\psi(z) dP(z)\n",
    "$$\n",
    "\n",
    "If we assume that $P = \\sum_{k=1}^s c_k Q_{\\theta_k}$ is a mixture model of template distributions $Q_\\theta$ parameterized by $\\theta$, then we find again\n",
    "$$\n",
    "y \\approx \\int \\phi(\\theta) d\\mu(\\theta) = \\sum_{k=1}^s c_k \\phi(\\theta_k)\n",
    "$$\n",
    "with $\\mu = \\sum_{k=1}^s c_k \\delta_{\\theta_k}$ a sparse distribution, and $\\phi(\\theta) = \\int \\psi(z) dQ_\\theta(z)$ the expectation of $\\psi$ wrt $Q_\\theta$. That's the previous problem. The big difference is that $\\theta$ can be very high-dimensional!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4810f",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "A simple example of this is Gaussian Mixture Model (GMM). Here\n",
    "$$\n",
    "Q_\\theta = \\mathcal{N}(\\theta, Id)\n",
    "$$\n",
    "is a Gaussian distribution with identity covariance for simplicity. If we consider again\n",
    "$$\n",
    "\\psi(z) = [e^{i w^\\top z}]_{w \\in T}\n",
    "$$\n",
    "for some set of (random, multi-dimensional) frequencies $T$, then $\\phi$ is the characteristic function of $Q_\\theta$:\n",
    "$$\n",
    "\\phi(\\theta) = [e^{i w^\\top \\theta} e^{-\\|w\\|_2^2 /2}]_{w \\in T}\n",
    "$$\n",
    "We give the code to generate the $z_i$, an appropriate $T$, and the signal $y$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55450a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "d = 2 # dimension\n",
    "s = 3 # sparsity\n",
    "true_thetas = 2*torch.randn(s, d)\n",
    "true_c = torch.tensor([.1, .3, .6]) # must sum to 1 !\n",
    "\n",
    "# sample data\n",
    "N = 2000\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='spherical')\n",
    "gmm.weights_ = true_c.numpy()\n",
    "gmm.means_ = true_thetas.numpy()\n",
    "gmm.covariances_ = np.ones(s)\n",
    "data = gmm.sample(N)\n",
    "labels = data[1]\n",
    "Z = torch.tensor(data[0], dtype=torch.float)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(Z.numpy()[:,0], Z.numpy()[:,1], c=labels, s=2)\n",
    "plt.scatter(true_thetas[:,0], true_thetas[:,1], marker='x', s=1000*true_c, c='r')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e408a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random frequencies\n",
    "m = 100\n",
    "T = torch.randn(m,d)/np.sqrt(d) # variance 1/d for good scaling\n",
    "\n",
    "y = torch.cat((\n",
    "            torch.cos(T @ Z.T),\n",
    "            torch.sin(T @ Z.T))).mean(axis=1)\n",
    "# Display by increasing norm of freq\n",
    "T_amp = (T**2).sum(axis=1)\n",
    "plt.figure()\n",
    "plt.scatter(T_amp, y[:m])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7093f2",
   "metadata": {},
   "source": [
    "**Exo**: adapt the *mat* and *obs_model* functions to the new $\\phi$. Compute the \"true\" value of $y^* = \\int \\phi(\\theta) d\\mu(\\theta)$. Plot both $y$ and $y^*$ and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a82b29",
   "metadata": {},
   "source": [
    "### Greedy approach\n",
    "\n",
    "In high dimension, it is \"impossible\" to perform the grid approach from earlier. Indeed, for a fixed precision, the size of the grid must be exponential with the dimension!\n",
    "\n",
    "One possibility is to take inspiration from greedy approaches and add the components one by one, for instance by maximizing their correlation with the residual signal, and alternate this with the global non-convex optimization step from earlier. This has been formulated several times in the literature, inspired by MP-type algorithm (eg [here](https://arxiv.org/abs/1606.02838)) or by Frank-Wolfe iterative algorithm for convex minimization (eg [here](https://arxiv.org/abs/1811.06416))\n",
    "\n",
    "In details, the algorithm reads\n",
    "- 1. Initialize the support $S$ to $\\emptyset$ and residual to $r = y$\n",
    "- 2. Solve\n",
    "$$\n",
    "\\hat \\theta = \\max_\\theta \\langle r , \\phi(\\theta)/\\|\\phi(\\theta)\\|_2 \\rangle\n",
    "$$\n",
    "and add $\\hat \\theta$ to the support: $S \\leftarrow S \\cup \\{\\hat \\theta\\}$\n",
    "- 3. Denote the current support $S = \\{\\hat \\theta_1, \\ldots, \\hat \\theta_{|S|}\\}$ and initialize the Least Square amplitudes\n",
    "$$\n",
    "\\hat c = \\arg\\min_{c} \\|y - \\sum_{k=1}^{|S|} c_k \\phi(\\hat \\theta_k) \\|_2^2\n",
    "$$\n",
    "- 4. Solve the non-convex optimization problem\n",
    "$$\n",
    "S, c = \\arg\\min_{S,c} \\|y - \\sum_{k=1}^{|S|} c_k \\phi(\\theta_k) \\|_2^2\n",
    "$$\n",
    "initialized with $\\hat S, \\hat c$.\n",
    "- 5. Update the residual $r = y - \\sum_{k=1}^{|S|} c_k \\phi(\\theta_k)$\n",
    "- 6. reiterate from 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e13ef",
   "metadata": {},
   "source": [
    "**Exo**: Implement the optimization from step 2 using Pytorch. Test it on a simple instance with $s=1$. Compare the recovered $\\theta$ with the average of the $z_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7f665",
   "metadata": {},
   "source": [
    "**Exo**: Implement the rest of the algorithm, adapting the non-convex optimization procedure from earlier and using torch.linalg.lstsq for Least Squares. Test it. Plot the data at each step of the algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
